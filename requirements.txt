guidance

# for GPU support, need to run
# CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --force-reinstall --upgrade --no-cache-dir llama-cpp-python
# https://github.com/abetlen/llama-cpp-python/issues/576#issuecomment-1766003289
llama-cpp-python